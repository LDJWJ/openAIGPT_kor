# p40

# p40

messages = [
  {
    "role": "user",
    "content":
    "electrons dance the tango of uncertainty," 
    "entangling bits in a choreography that outpaces" 
    "the swiftest supercomputers."
  },
  {
    "role": "assistant",
    "content":
    "Electrons Dance the Tango of Uncertainty, " 
    "Entangling Bits in a Choreography That Outpaces " 
    "the Swiftest Supercomputers."
  },
  {
    "role": "user",
    "content":
    "cloud architectures whisper across the sky, " 
    "weaving a tapestry of data that blankets the digital " 
    "landscape in a seamless symphony of bytes."
  },
  {
    "role": "assistant",
    "content":
    "Cloud Architectures Whisper Across the Sky, " 
    "Weaving a Tapestry of Data That Blankets the Digital " 
    "Landscape in a Seamless Symphony of Bytes." 
  },
  {
    "role": "user",
    "content":
    "artificial Intelligence, the alchemist of the digital " 
    "age, transmutes raw data into a golden labyrinth of " 
    "insights, charting new territories in the realm of " "human thought."
  },
  {
    "role": "assistant",
    "content":
    "Artificial Intelligence, the Alchemist of the Digital " 
    "Age, Transmutes Raw Data Into a Golden Labyrinth of " 
    "Insights, Charting New Territories in the Realm of " "Human Thought."
  },
  {
    "role": "user",
    "content":
    "the internet of things is a vast ocean of data, " 
    "a sea of information that ebbs and flows " "with the tides of time."
  } 
]

response = client.chat.completions.create( 
    model=model, 
    messages=messages, 
    temperature=1.2,
)

print(response.choices[0].message.content) 


# p42
model = "gpt-4"
messages = [
  {
    "role": "user", 
    "content":
      "Return a JSON containing " 
      "the primary numbers between 0 and 3." 
  },
  {
    "role": "assistant", 
  "content": """
    {
      "data": [2, 3, 5, 7], 
      "length": 4, 
      "smallest": 2, 
      "largest": 7,
    }
    """
  },
  {
    "role": "user", 
    "content":
      "Return a JSON containing " 
      "the primary numbers between 0 and 6." },
  {
    "role": "assistant", 
    "content": """
      {
        "data": [2, 3, 5], 
        "length": 3, 
        "smallest": 2, 
        "largest": 5,
      }
    """
  },
  {
    "role": "user", 
    "content":
      "Return a JSON containing " 
      "the primary numbers between 11 and 65."
  } 
]

response = client.chat.completions.create( 
    model=model, 
    messages=messages,
)

print(response.choices[0].message.content) 



# p43


model = "gpt-4"
prefix = "\n\n1. "
messages = [
  {
    "role": "user",
    "content":
      f"What are the 7 wonders of the world?{prefix}" 
  },
]

response = client.chat.completions.create( 
    model=model, 
    messages=messages,
)

print(prefix + response.choices[0].message.content) 


# p44

model = "gpt-3.5-turbo"
messages = [
{
"role": "system", "content":
"You are a smart and creative assistant." },
{
"role": "user",
"content": "Who is Hannibal?" },
]

response = client.chat.completions.create( model=model, messages=messages,
)
# print the usage print(response.usage) 



# p45

model = "gpt-3.5-turbo"
messages = [
  {
    "role": "system", "content":
    "You are a smart and creative assistant." 
  },
  {
    "role": "user",
    "content": "Who is Hannibal?" 
  },
]

response = client.chat.completions.create( 
    model=model, 
    messages=messages,
)

# print the usage 
print(response.usage) 



# p46


model = "gpt-3.5-turbo"
messages = [
  {
    "role": "system", "content":
    "You are a smart and creative assistant." 
  },
  {
    "role": "user",
    "content":
    "Who is Hannibal?"
  },
]

short_response = client.chat.completions.create( 
    model=model, 
    messages=messages, 
    max_tokens=50,
)

long_response = client.chat.completions.create( 
    model=model, 
    messages=messages, 
    max_tokens=300,
)


print("Short response:") 
print(short_response.choices[0].message.content) 
print()
print("Long response:") 
print(long_response.choices[0].message.content)



# p48

model = "gpt-3.5-turbo"
messages = [
  {
    "role": "system", "content":
    "You are a smart and creative assistant." 
  },
  {
    "role": "user",
    "content":
    "Who is Hannibal?"
  },
]

stop_token = "."
response = client.chat.completions.create( 
    model=model, 
    messages=messages, 
    max_tokens=50, 
    stop=[stop_token],
)

print(response.choices[0].message.content + stop_token)


# p49


model = "gpt-3.5-turbo"
messages = [
  {
    "role": "system", "content":
    "You are a smart and creative assistant." 
  },
  {
    "role": "user",
    "content":
    "Who is Hannibal?"
  },
]

response = client.chat.completions.create( 
    model=model, 
    messages=messages, 
    max_tokens=300, 
    stop=["\n", "Human:", "AI:"],
)

print(response.choices[0].message.content)



# p50


model = "gpt-3.5-turbo"
messages = [
  {
    "role": "user",
    "content":
      "Name sci-fi movies that came out in 2021." 
  },
  {
    "role": "system",
    "content": """
      1. Dune
      2. Finch
      3. The Awake
      4. The Matrix Resurrections 5. Mother/Android
      6. Bliss
      7. Swan Song
    """
  },
  {
    "role": "user",
    "content":
      "Name fantasy movies that came out in 2021." 
  },
]

response = client.chat.completions.create( 
    model=model, 
    messages=messages,
    max_tokens=300, 
    stop=["Human:", "AI:"],
)

print(response.choices[0].message.content)



# p52

model = "gpt-3.5-turbo"
messages = [
  {
    "role": "user",
    "content":
      "Name sci-fi movies that came out in 2021." 
  },
  {
    "role": "system",
    "content": """
      1. Dune
      2. Finch
      3. The Awake
      4. The Matrix Resurrections 5. Mother/Android
      6. Bliss
      7. Swan Song
    """
  },
  {
    "role": "user",
    "content":
      "Name fantasy movies that came out in 2021." 
  },
]

response = client.chat.completions.create( 
    model=model, 
    messages=messages,
    max_tokens=300, 
    stop=["6."],
)

print(response.choices[0].message.content)

# p53

model = "gpt-4"
prefix = "Once upon a time " 

messages = [
{
  "role": "system", 
  "content": "You are a storyteller." 
},
{
  "role": "user", 
  "content": prefix
},
]

response_high_temperature = client.chat.completions.create( model=model,
  messages=messages,
  max_tokens=100,
  temperature=2,
  stop=["\n",],
)

content_high_temperature = \ 
  response_high_temperature.choices[0].message.content

response_medium_temperature = client.chat.completions.create( 
  model=model,
  messages=messages,
  max_tokens=100,
  temperature=1,
  stop=["\n",],
)

content_medium_temperature = \ 
  response_medium_temperature.choices[0].message.content

response_low_temperature = client.chat.completions.create( 
  model=model,
  messages=messages,
  max_tokens=100,
  temperature=0,
  stop=["\n",],
)

content_low_temperature = \ 
  response_low_temperature.choices[0].message.content

print(f"""
1. High temperature: 
    {prefix}{content_high_temperature}
""")

print(f"""
2. Medium temperature: 
    {prefix}{content_medium_temperature}
""")

print(f"""
3. Low temperature: 
    {prefix}{content_low_temperature}
""") 


# p56

model = "gpt-4"
prefix = "Once upon a time " 
messages = [
  {
    "role": "system", 
    "content": "You are a storyteller." 
  },
  {
    "role": "user", 
    "content": prefix
  },
]

response_high_topp = client.chat.completions.create( model=model,
messages=messages, max_tokens=100,
top_p=1,
stop=["\n",],
)
content_high_topp = \
    response_high_topp.choices[0].message.content

response_medium_topp = client.chat.completions.create( 
    model=model,
    messages=messages, 
    max_tokens=100,
    temperature=1,
    stop=["\n",],
)

content_medium_topp = \
    response_medium_topp.choices[0].message.content

response_low_topp = client.chat.completions.create( 
    model=model, 
    messages=messages, 
    max_tokens=100,
    temperature=0,
    stop=["\n",],
)

content_low_topp = \
    response_low_topp.choices[0].message.content

print(f"""
1. High topp: 
    {prefix}{content_high_topp}
""")

print(f"""
2. Medium topp: 
    {prefix}{content_medium_topp}
""")

print(f"""
3. Low topp: 
    {prefix}{content_low_topp}
""")



### p59


model = "gpt-4"
prefix = "Once upon a time " 
messages = [
  {
    "role": "system", 
    "content": "You are a storyteller." 
  },
  {
    "role": "user", 
    "content": prefix 
  },
]

response = client.chat.completions.create( 
    model=model, 
    messages=messages, 
    max_tokens=100,
    # set this to True to enable streaming 
    stream=True,
)

print(prefix, end="")
for message in response: 
  content = message.choices[0].delta.content 
  if content: 
    print(content, end="")


### Controlling Repetitivity: Frequency and Presence Penalties
# p60

model = "gpt-3.5-turbo" 
prefix = "Once upon a time " 
messages = [
{
  "role": "system", 
  "content": "You are a storyteller." 
},
{
  "role": "user", 
  "content": prefix
},
]
response_high_frequency_penalty = client.chat.completions.create( 
    model=model,
    messages=messages,
    max_tokens=100,
    frequency_penalty=2.0,
)

response_low_frequency_penalty = client.chat.completions.create( 
    model=model,
    messages=messages,
    max_tokens=100,
    frequency_penalty=0,
)

response_high_presence_penalty = client.chat.completions.create( 
    model=model,
    messages=messages,
    max_tokens=100,
    presence_penalty=2.0,
)

response_low_presence_penalty = client.chat.completions.create( 
    model=model,
    messages=messages,
    max_tokens=100,
    presence_penalty=0,
)

content_high_frequency_penalty = \
    response_high_frequency_penalty.choices[0].message.content
content_low_frequency_penalty = \
    response_low_frequency_penalty.choices[0].message.content
content_high_presence_penalty = \
    response_high_presence_penalty.choices[0].message.content
content_low_presence_penalty = \
    response_low_presence_penalty.choices[0].message.content

print("High frequency penalty:") 
print(prefix + content_high_frequency_penalty) 
print()
print("Low frequency penalty:") 
print(prefix + content_low_frequency_penalty) 
print()
print("High presence penalty:") 
print(prefix + content_high_presence_penalty)
print()
print("Low presence penalty:") 
print(prefix + content_low_presence_penalty) 


### Frequency vs. Presence Penalty
# p62

### Controlling the Number of Results from the API - p63

model = "gpt-4"
prefix = "Once upon a time " 

messages = [
{
    "role": "system", 
    "content": "You are a storyteller." },
{
    "role": "user", 
    "content": prefix, },
]

response = client.chat.completions.create( 
    model=model, 
    messages=messages,
    n = 2,
    stop=["\n"]
)
choices = response.choices 
for choice in choices: 
    print(f"Choice: {choice.index}") 
    print(prefix + choice.message.content) 
    print()


### Few Shot Learning: A Key Prompt Engineering Technique - p67




model = "gpt-4"
messages = [
  {
    "role": "system",
    "content":
      "You are a helpful bot that helps " 
      "people extract keywords from a text." 
      "Keywords are words that are important " 
      "in a text."
  },
  {
    "role": "user",
    "content":
      "In a whimsical town, old tortoise Orion " 
      "scattered sunflower seeds atop a hill, " 
      "believing they carried wishes to the stars. " 
      "Unknown to him, children below rejoiced, " 
      "believing these seeds were blessings from " 
      "the sky. Sometimes, simple acts create magic." 
  },
  {
    "role": "assistant",
    "content":
      "[\"whimsical town\", \"old tortoise\"," 
      "\"Orion\", \"sunflower seeds\"," 
      "\"hill\", \"wishes\", \"stars\", " 
      "\"children\", \"blessings\", \"sky\"," 
      "\"simple acts\", \"magic\"]"

  },
  {
    "role": "user",
    "content":
    "In the world of quantum computing, the Hadron " 
    "Processing Unit (HPU) stands out. Utilizing " 
    "qubit superposition, it offers exceptional " 
    "speeds. Paired with nanophotonic circuits, " 
    "it ensures fast qubit communication, while " 
    "quantum tunneling aids in error correction. " 
    "With quantum annealing algorithms, the HPU " 
    "brings us closer to achieving quantum supremacy." 
  },
  {
    "role": "assistant",
    "content":
      "[\"quantum computing\", \"Hadron Processing Unit\", "
      "\"HPU\", \"qubit superposition\", \"speeds\",  "
      "\"nanophotonic circuits\", \"qubit communication\",  "
      "\"quantum tunneling\", \"error correction\",  "
      "\"quantum annealing algorithms\",  "
      "\"quantum supremacy\"] "
  },
  {
    "role": "user",
    "content":
      "The first programming language to be invented was " 
      "Plankalkül, which was designed by Konrad Zuse in " 
      "the 1940s, but not publicly known until 1972 " 
      "(and not implemented until 1998). The first " 
      "widely known and successful high-level programming " 
      "language was Fortran, developed from 1954 to 1957 " 
      "by a team of IBM researchers led by John Backus. " 
      "The success of FORTRAN led to the formation of a " 
      "committee of scientists to develop a \"universal\" " 
      "computer language; the result of their effort was " 
      "ALGOL 58. Separately, John McCarthy of MIT developed " 
      "Lisp, the first language with origins in academia " 
      "to be successful. With the success of these initial " 
      "efforts, programming languages became an active " 
      "topic of research in the 1960s and beyond." 
  },
]

response = client.chat.completions.create( 
      model=model, 
      messages=messages, 
      max_tokens=100, 
      temperature=0,
)

print(response.choices[0].message.content)


# p71

# p71

import sys

print("Please provide the name of the crypto as an argument.")
crypto = input("(ex)cardano, bitcoin")

model = "gpt-4"

# The name of the crypto is passed as an argument 
# (for example: python3 crypto.py cardano) 

messages = [
  {
    "role": "system", 
    "content": "You are a smart assistant.", },
  {
    "role": "user", 
    "content": "Bitcoin",
  },
  {
    "role": "assistant", 
    "content":
      "- BTC was created in 2008.\n"
      "- You can learn more about it here:\n" 
      "https://bitcoin.org/en/\n" 
      "- You can get the latest price here:\n" 
      "https://www.coingecko.com/en/coins/bitcoin\n" 
      "- Its all-time high is $64,895.00.\n" 
      "- Its all-time low is $67.81.\n" },
  {
    "role": "user",
    "content": "Ethereum",
  },
  {
    "role": "assistant",
    "content":
      "- ETH was created in 2015.\n" 
      "- You can learn more about it here:\n" 
      "https://ethereum.org/en/\n" 
      "- You can get the latest price here:\n" 
      "https://www.coingecko.com/en/coins/ethereum\n" 
      "- Its all-time high is $4,362.35.\n" 
      "- Its all-time low is $0.43.\n" },
  {
    "role": "user",
    "content": "Dogecoin",
  },
  {
    "role": "assistant",
    "content":
      "- DOGE was created in 2013.\n" 
      "- You can learn more about it here:\n" 
      "https://dogecoin.com/\n"
      "- You can get the latest price here:\n" 
      "https://www.coingecko.com/en/coins/dogecoin\n" 
      "- Its all-time high is $0.73.\n" 
      "- Its all-time low is $0.00008690.\n" 
  },
  {
    "role": "user",
    "content": crypto,
  }
]

response = client.chat.completions.create( 
    model=model, 
    messages=messages,
)

output = response.choices[0].message.content.strip() 
print(output)


### Overgeneration and Selection - p73



model = "gpt-4"
# First prompt to hashtags into a Python list 
messages = [
  {
    "role": "system", 
    "content":
      "You are a helpful bot that helps people " 
      "extract hashtags from text to use them " 
      "on Twitter.",
  },
  {
    "role": "user",
    "content":
      "In a whimsical town, old tortoise Orion " 
      "scattered sunflower seeds atop a hill, " 
      "believing they carried wishes to the " 
      "stars. Unknown to him, children below "
      "rejoiced, believing these seeds were " 
      "blessings from the sky. Sometimes, " 
      "simple acts create magic."
  },
  {
    "role": "assistant",
    "content": "#Wishes #Blessings #Magic", 
  },
  {
    "role": "user",
    "content":
      "In the world of quantum computing, " 
      "the Hadron Processing Unit (HPU) " 
      "stands out. Utilizing qubit superposition, " 
      "it offers exceptional speeds. Paired with " 
      "nanophotonic circuits, it ensures fast qubit " 
      "communication, while quantum tunneling aids " 
      "in error correction. With quantum annealing " 
      "algorithms, the HPU brings us closer to " 
      "achieving quantum supremacy."
  },
  {
    "role": "assistant",
    "content":
      "#QuantumComputing #Qubit #QuantumAlgorithms", 
  },
  {
    "role": "user",
    "content":
      "What makes a good friend is someone who is " 
      "there for you when you need them. They are " 
      "someone who will listen to your problems " 
      "and help you find solutions. They are " 
      "someone who will be there for you " 
      "when you need them most."
  },
  {
    "role": "assistant",
    "content": "#Friendship #FriendshipQuotes #Support", 
  },
  {
    "role": "user",
    "content":
      "The first programming language to be invented " 
      "was Plankalkül, which was designed by Konrad " 
      "Zuse in the 1940s, but not publicly known until " 
      "1972 (and not implemented until 1998). The first " 
      "widely known and successful high-level programming " 
      "language was Fortran, developed from 1954 to 1957 " 
      "by a team of IBM researchers led by John Backus. " 
      "The success of FORTRAN led to the formation of " 
      "a committee of scientists to develop a " 
      "\"universal\" computer language; the result of " 
      "their effort was ALGOL 58. Separately, John McCarthy " 
      "of MIT developed Lisp, the first language with " 
      "origins in academia to be successful. With the " 
      "success of these initial efforts, programming " 
      "languages became an active topic of research " 
      "in the 1960s and beyond."
  },
]

response = client.chat.completions.create( 
    model=model, 
    messages=messages, 
    max_tokens=100, 
    temperature=0, 
    stop=["\n", "assistant:", "user:"],
)

hashtags = response.choices[0].message.content

# Second prompt to generate a tweet 
# By giving the hashtags as input, the model 
# will generate a tweet. Examples are provided 
# in the messages list to give the model 
# some guidance about the task especially the 
# length of the tweet but you can also guide it 
# through a special style/tone of writing.

messages = [
  {
    "role": "system",
    "content":
      "You are an intelligent assistant that assists " 
      "users in creating a tweet with hashtags based "
      "on a given text. The tweet should be between " 
      "100 and 280 characters in length."
  },
  {
    "role": "user",
    "content":
      "OpenAI has been pushing the boundaries of large " 
      "language models, making them more accessible to " 
      "the public and businesses alike. The models can " 
      "generate human-like text, assist in writing, " 
      "answer questions, and more. " 
      "#OpenAI #AI #LanguageModels"
  },
  {
    "role": "assistant",
    "content":
      "OpenAI advances large language models for public " 
      "and business use. They generate human-like text, " 
      "assist writing, and answer questions. " 
      "#OpenAI #AI #LanguageModels"
  },
  {
    "role": "user",
    "content":
      "The Eiffel Tower, located in Paris, France, " 
      "is a world-renowned architectural marvel and " 
      "a symbol of love. Many tourists visit it annually " 
      "to witness its grandeur and romantic ambiance. " 
      "#EiffelTower #Paris #Romance"
  },
  {
    "role": "assistant",
    "content":
      "Paris's Eiffel Tower is an iconic symbol of love, " 
      "drawing tourists worldwide to its majestic " 
      "presence. "
      "#EiffelTower #Paris #Love"
  },
  {
    "role": "user",
    "content":
      "Yoga is an ancient practice originating from India. " 
      "It focuses on mental, physical, and spiritual "
      "well-being, offering exercises and meditations " 
      "that promote holistic health. " 
      "#Yoga #Wellness #Meditation"
  },
  {
    "role": "assistant",
    "content":
      "Yoga, from India, balances mind, body, and spirit " 
      "through exercises and meditation. " 
      "#Yoga #HolisticHealth #Meditation"
  },
  {
    "role": "user",
    "content":
      "Pandas are native to South Central China and are known " 
      "for their distinct black and white appearance. They " 
      "mostly eat bamboo and are a symbol of peace and " 
      "conservation efforts. "
      "#Pandas #China #Conservation"
  },
  {
    "role": "assistant",
    "content":
      "Native to China, pandas with their iconic black and " 
      "white look are symbols of peace and conservation. " 
      "#Pandas #China #Peace"
  },
  {
    "role": "user",
    "content":
      "The Grand Canyon is a steep-sided canyon carved by " 
      "the Colorado River in the U.S. state of Arizona. " 
      "It's one of the most famous landmarks and is known " 
      "for its immense size and its intricate and colorful " 
      "landscape. "
      "#GrandCanyon #Arizona #Nature"
  },
  {
    "role": "assistant",
    "content":
      "The Grand Canyon, carved by the Colorado River in " 
      "Arizona, is famous for its immense size and colorful " 
      "landscape. "
      "#GrandCanyon #Arizona #Nature"
  },
  {
    "role": "user",
    "content":
      "The first programming language to be invented was " 
      "Plankalkül, which was designed by Konrad Zuse in " 
      "the 1940s, but not publicly known until 1972 " 
      "(and not implemented until 1998). The first widely " 
      "known and successful high-level programming language " 
      "was Fortran, developed from 1954 to 1957 by a team " 
      "of IBM researchers led by John Backus. The success " 
      "of FORTRAN led to the formation of a committee of " 
      "scientists to develop a \"universal\" computer " 
      "language; the result of their effort was ALGOL 58. " 
      "Separately, John McCarthy of MIT developed Lisp, " 
      "the first language with origins in academia to be " 
      "successful. With the success of these initial efforts, " 
      "programming languages became an active topic of " 
      f"research in the 1960s and beyond. {hashtags}" 
  },
]

# We generate 5 tweets and select the first 
# one that is less than 280 characters 
response = client.chat.completions.create( 
    model=model, 
    messages=messages, 
    max_tokens=80, 
    temperature=0.5, 
    stop=["\n", "assistant:", "user:"], 
    n=5
)

for choice in response.choices:
  tweet = choice.message.content
  length = len(tweet) 
  if length <= 280: 
    print(tweet) 
    print() 
    break


# p82


model = "gpt-4"
# First prompt to generate knowledge 

prompt = """
  Write a concise paragraph about 
  the lyrical characteristics 
  and themes of old-school rap. 
  """

messages = [
  {
    "role": "system", 
    "content": "You are a smart assistant.", 
  },
  {
    "role": "user", 
    "content": prompt,
  }
]

response = client.chat.completions.create( 
    model=model, 
    messages=messages, 
    max_tokens=200, 
    temperature=0.5, 
    stop=["assistant:", "user:"],
)

output = response.choices[0].message.content

# Second prompt to generate the lyrics 
# By feeding the output of the first prompt 
# as the input of the second prompt, 
# we can generate a better response 
prompt = f"""Context: {output}
Task: Create lyrics for an old-school 
rap song about justice and equality. 
"""

messages = [
  {
    "role": "system",
    "content":
      "You are a popular old-school rap lyricist.", 
  },
  {
    "role": "user",
    "content": prompt,
  },
]

# We generate 5 tweets and select 
# the first one that is less 
# than 280 characters 
response = client.chat.completions.create( 
    model=model, 
    messages=messages, 
    max_tokens=500, 
    temperature=1, 
    stop=["assistant:", "user:"],
)

output = response.choices[0].message.content 

print(
  "This is the prompt we fed to the model" 
  f":\n\n{prompt}"
)

print()
print(
  f"This is the result:\n\n{output}" 
)



### Context Stuffing: Is Apple a Fruit or a Company? - 85


model = "gpt-3.5-turbo"
prompt = "Determine the part of speech of the word 'light'." 

messages = [
  {
    "role": "system",
    "content": "You are a smart assistant.", 
  },
  {
    "role": "user",
    "content": prompt,
  }
]

response = client.chat.completions.create(
  model=model, 
  messages=messages,
)

output = response.choices[0].message.content 
print(output)


### p87


model = "gpt-3.5-turbo"
prompt_a = """The light is red. Determine the part 
of speech of the word 'light'.\n\n"""

prompt_b = """This desk is very light. Determine 
the part of speech of the word 'light'.\n\n"""

prompt_c = """You light up my life. Determine the 
part of speech of the word 'light'.\n\n"""

prompt_d = """He stepped light on the snow, trying 
not to leave deep footprints. Determine the part of 
speech of the word 'light'.\n\n"""

for prompt in [
  prompt_a,
  prompt_b,
  prompt_c,
  prompt_d
]:
  messages = [
  {
    "role": "system", 
    "content": "You are a smart assistant.", },
  {
    "role": "user", 
    "content": prompt,
  }
  ]

response = client.chat.completions.create( 
    model=model, 
    messages=messages,
)

output = response.choices[0].message.content 
print(output)
print()

#p89

model = "gpt-3.5-turbo" 
prompt_a = """ 
Huawei: company
Google: company
Microsoft: company
Apple: 
""" 

prompt_b = """ 
Huawei: company
Google: company
Microsoft: company
Apricot: Fruit
Apple: 
"""

for prompt in [prompt_a, prompt_b]: 
  messages = [
    {
      "role": "system", 
      "content": "You are a smart assistant.", 
    },
    {
      "role": "user", 
      "content": prompt, 
    }
  ]

response = client.chat.completions.create( 
    model=model, 
    messages=messages,
)

output = response.choices[0].message.content 
print(output)
print()


# p90


model = "gpt-4"
# Prompt for generating a Dockerfile for a 
# Python application 

prompt_dockerfile = """
# Dockerfile for Node.js: FROM node:14
WORKDIR /app
COPY . /app
RUN npm install
EXPOSE 8080
CMD ["node", "app.js"]

# Dockerfile for Python: 
"""

# Prompt for generating a Kubernetes deployment 
# script for a MySQL database

prompt_kubernetes = """ 
# Kubernetes deployment for Redis: 
apiVersion: apps/v1 
kind: Deployment 
metadata:
  name: redis-deployment 
spec:
  selector: 
    matchLabels: 
      app: redis 
  template: 
    metadata: 
      labels: 
        app: redis 
    spec: 
      containers: 
      - name: redis 
        image: redis 
        ports: 
        - containerPort: 6379

# Kubernetes deployment for MySQL: 
"""
for prompt in [ 
    prompt_dockerfile, 
    prompt_kubernetes
]:
  messages = [
    {
    "role": "system", 
    "content":
      "You are a smart assistant who writes " 
      "configuration scripts.", 
    },
    {
      "role": "user", 
      "content": prompt,
    }
  ]

response = client.chat.completions.create( 
    model=model,
    messages=messages,
)

output = response.choices[0].message.content 
print(output.strip()) 
print("---")

### Dynamic Max Tokens - p92

import sys
model = "gpt-4"

# Read the number of tasks from the command line 
print("Please provide the number of tasks")
number_of_tasks = int( input("(ex) 5, 10 .. : ") )

# Create a prompt to guide the model to create 
# a to-do list with a given format 
prompt = """
Create a to-do list to create a company in US

Task 1: 
"""

# Define the stop sequence.
# If the user want to generate 5 tasks, 
# the stop sequence should include "Task 6:" 

stop = [
  f"Task {number_of_tasks + 1}:", 
  "assistant:",
  "user:"
]

messages = [
  {
    "role": "system", 
    "content": "You are a smart assistant.", 
  },
  {
    "role": "user", 
    "content": prompt,
  }
]
response = client.chat.completions.create( 
    model=model, 
    messages=messages, 
    max_tokens=200, 
    stop=stop,
)

output = response.choices[0].message.content
# Concatenate "Task 1:" and the output 
full_response = "Task 1: " + output

print(full_response) 


### p95
import sys
model = "gpt-4"
# Read the number of tasks from the command line try:
number_of_tasks =
int(sys.argv[1])
except:
print("Please provide the number of tasks " "as an argument.\n Examples:\n" "python src/app.py 5" "python src/app.py 10"
)
exit()
# Create a prompt to guide the model # to create a to-do list with a given format prompt = """
Create a to-do list to create a company in US
Task 1: """
# Define the stop sequence.
# If the user want to generate 5 tasks, # the stop sequence should include "Task 6:" stop = [
f"Task {number_of_tasks + 1}:", "assistant:", "user:"
]
# Define the max tokens. max_tokens = number_of_tasks * 113 + 17
messages = [
{
"role": "system", "content": "You are a smart assistant.", },
{
"role": "user", "content": prompt, }
]
response = client.chat.completions.create( model=model, messages=messages, max_tokens=max_tokens, stop=stop,
)
output = response.choices[0].message.content
# Concatenate "Task 1:" and the output full_response = "Task 1: " + output
print(full_response)



# p95

import sys
model = "gpt-4"

# Read the number of tasks from the command line try:
print("Please provide the number of tasks")
number_of_tasks = int( input("(ex) 5, 10 .. : ") )


# Create a prompt to guide the model # to create a to-do list with a given format 
prompt = """
Create a to-do list to create a company in US

Task 1: 
"""

# Define the stop sequence.
# If the user want to generate 5 tasks, 
# the stop sequence should include "Task 6:" 
stop = [
  f"Task {number_of_tasks + 1}:", 
  "assistant:", 
  "user:"
]

# Define the max tokens. 
max_tokens = number_of_tasks * 113 + 17

messages = [
  {
    "role": "system", 
    "content": "You are a smart assistant.", 
  },
  {
    "role": "user", 
    "content": prompt, 
  }
]

response = client.chat.completions.create( 
    model=model, 
    messages=messages, 
    max_tokens=max_tokens, 
    stop=stop,
)

output = response.choices[0].message.content

# Concatenate "Task 1:" and the output 
full_response = "Task 1: " + output

print(full_response)


### Creating an Interactive CLI-Based Assistant - p96

model = "gpt-4"
messages = [
  {
    "role": "system", "content":
    "You are a smart assistant." 
  },
  {
    "role": "user", "content":
    "List all the files in the current " "directory."
  },
  {
    "role": "assistant", "content":
    "ls -l"
  },
  {
    "role": "user", "content":
    "List all the files in the current " "directory, including hidden files." 
  },
  {
    "role": "assistant", "content":
    "ls -la"
  },
  {
    "role": "user", "content":
    "Delete all the files in the current " "directory."
  },
  {
    "role": "assistant", "content":
    "rm *"
  },
  {
    "role": "user",
    "content":
    "Count the number of occurrences of " "the word 'sun' in the file 'test.txt'." 
  },
  {
    "role": "assistant", "content":
    "grep -o 'sun' test.txt | wc -l" 
  },
  {
    "role": "user",
    "content":
    "Count the number of files in the current " "directory."
  } 
]

response = client.chat.completions.create( 
    model=model, 
    messages=messages, 
    max_tokens=200, 
    temperature=0,
)

output = response.choices[0].message.content.strip() 

print(output)

# p99

import click
model = "gpt-4"
base_messages = [
  {
    "role": "system", 
    "content":
      "You are a smart assistant." 
  },
  {
    "role": "user", 
    "content":
      "List all the files in the current " 
      "directory."
  },
  {
    "role": "assistant", 
    "content":
      "ls -l"
  },
  {
    "role": "user", 
    "content":
      "List all the files in the current " 
      "directory, including hidden files." 
  },
  {
    "role": "assistant", 
    "content":
      "ls -la"
  },
  {
    "role": "user",
    "content":
      "Delete all the files in the current " 
      "directory."
  },
  {
    "role": "assistant", 
    "content":
      "rm *"
  },
  {
    "role": "user", 
    "content":
      "Count the number of occurrences of " 
      "the word 'sun' in the file 'test.txt'." 
  },
  {
    "role": "assistant", 
    "content":
      "grep -o 'sun' test.txt | wc -l" 
  },
]

while True: 
  messages = base_messages.copy()

  # read the user input 
  request = input( 
      click.style( 
          "Input (type 'exit' to quit): ",  
          fg="green" 
      ) 
  )

  if request.lower() in ["exit", "quit"]: 
    break


  # add the user input to the messages 
  messages.append( 
      {
        "role": "user", 
        "content": f"{request}"
      }
  )

  # send the messages to the API 
  response = client.chat.completions.create(
      model=model, 
      messages=messages, 
      max_tokens=200, 
      temperature=0,
  )

  # get the command from the response 
  command = response.choices[0].message.content.strip()

  # Print the command in a nice way 
  click.echo( 
      click.style( 
          "Output: ", fg="yellow" 
      ) + command
  )
  click.echo()

### p102

import click
model = "gpt-4"
base_messages = [
  {
    "role": "system", 
    "content":
      "You are a smart assistant." 
  },
  {
    "role": "user", 
    "content":
      "List all the files in the current " 
      "directory."
  },
  {
    "role": "assistant", 
    "content":
      "ls -l"
  },
  {
    "role": "user", 
    "content":
      "List all the files in the current " 
      "directory, including hidden files." 
  },
  {
    "role": "assistant", 
    "content":
      "ls -la"
  },
  {
    "role": "user",
    "content":
      "Delete all the files in the current " 
      "directory."
  },
  {
    "role": "assistant", 
    "content":
      "rm *"
  },
  {
    "role": "user", 
    "content":
      "Count the number of occurrences of " 
      "the word 'sun' in the file 'test.txt'." 
  },
  {
    "role": "assistant", 
    "content":
      "grep -o 'sun' test.txt | wc -l" 
  },
]

while True: 
  messages = base_messages.copy()

  # read the user input 
  request = input( 
      click.style( 
          "Input (type 'exit' to quit): ",  
          fg="green" 
      ) 
  )

  if request.lower() in ["exit", "quit"]: 
    break


  # add the user input to the messages 
  messages.append( 
      {
        "role": "user", 
        "content": f"{request}"
      }
  )

  # send the messages to the API 
  response = client.chat.completions.create(
      model=model, 
      messages=messages, 
      max_tokens=200, 
      temperature=0,
  )

  # get the command from the response 
  command = response.choices[0].message.content.strip()

  # Print the command in a nice way 
  click.echo( 
      click.style( 
          "Output: ", fg="yellow" 
      ) + command
  )

  # Ask the user if they want to execute the command 
  click.echo(
    click.style(
      "Execute? (y/n): ", 
      fg="yellow"),
      nl=False
  )

  # Read the user input 
  choice = input()
  
  # If the user wants to execute the command, execute it 
  if choice == "y": 
    os.system(command)
  elif choice == "n":
    continue
  else:
    click.echo(
      click.style(
        "Invalid choice. Please enter 'y' or 'n'.",
        fg="red" 
      ) 
    )
    
  click.echo() 



# p105 - 에러 처리 추가

import click
model = "gpt-4"
base_messages = [
  {
    "role": "system", 
    "content":
      "You are a smart assistant." 
  },
  {
    "role": "user", 
    "content":
      "List all the files in the current " 
      "directory."
  },
  {
    "role": "assistant", 
    "content":
      "ls -l"
  },
  {
    "role": "user", 
    "content":
      "List all the files in the current " 
      "directory, including hidden files." 
  },
  {
    "role": "assistant", 
    "content":
      "ls -la"
  },
  {
    "role": "user",
    "content":
      "Delete all the files in the current " 
      "directory."
  },
  {
    "role": "assistant", 
    "content":
      "rm *"
  },
  {
    "role": "user", 
    "content":
      "Count the number of occurrences of " 
      "the word 'sun' in the file 'test.txt'." 
  },
  {
    "role": "assistant", 
    "content":
      "grep -o 'sun' test.txt | wc -l" 
  },
]

while True: 
  messages = base_messages.copy()

  # read the user input 
  request = input( 
      click.style( 
          "Input (type 'exit' to quit): ",  
          fg="green" 
      ) 
  )

  if request.lower() in ["exit", "quit"]: 
    break


  # add the user input to the messages 
  messages.append( 
      {
        "role": "user", 
        "content": f"{request}"
      }
  )

  # send the messages to the API 
  response = client.chat.completions.create(
      model=model, 
      messages=messages, 
      max_tokens=200, 
      temperature=0,
  )

  # get the command from the response 
  command = response.choices[0].message.content.strip()

  # Print the command in a nice way 
  click.echo( 
      click.style( 
          "Output: ", fg="yellow" 
      ) + command
  )

  # Ask the user if they want to execute the command 
  click.echo(
    click.style(
      "Execute? (y/n): ", 
      fg="yellow"),
      nl=False
  )

  # Read the user input 
  choice = input()
  
  # If the user wants to execute the command, execute it 
  if choice == "y":
    r = os.system(command)
    if r != 0:
      click.echo(
        click.style(
          "Error executing command.", 
          fg="red"
        )
      )

  # If the user doesn't want to execute the command, continue 
  elif choice == "n":
    continue
  else:
    click.echo(
      click.style(
        "Invalid choice. Please enter 'y' or 'n'.", 
        fg="red"
      )
    )

  click.echo() 
