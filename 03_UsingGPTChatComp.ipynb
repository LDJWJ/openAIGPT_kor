{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 03. GPT Chat Completion의 활용"
      ],
      "metadata": {
        "id": "R2zcFvCycxgE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 실습 내용\n",
        " * oeenAI 다양한 인공지능 모델 확인해 보기(API 호출 응답)"
      ],
      "metadata": {
        "id": "EdJ4V2DtdQY5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### chatgpt.env 환경파일 준비\n",
        " * 일반적으로 환경 변수는 .env 파일에 저장되지만, 구글 코랩 사용자의 편의를 위해 이 책에서는 chatgpt.env를 사용합니다.\n",
        " * 실제 개발 환경에서는 보통 .env를 사용하니, 이 점을 기억해 두세요."
      ],
      "metadata": {
        "id": "uV_GfDI3zFRS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 사전 준비\n",
        " * 구글 코랩 환경은 일정 시간이후에 초기화가 되기 때문에 두가지 작업을 매번 수행해야 함.\n",
        "   * chatgpt.env 파일 생성이 필요.\n",
        "     * 준비된 chatgpt.env를 내용을 변경하여 업로드 하거나 또는 API_KEY와 ORG_ID를 확인하여 생성한다.\n",
        "   * pip install openai 설치"
      ],
      "metadata": {
        "id": "IhueHTFAdFzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "-8fVYFLZdQAI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3e886a3-35d5-419a-d568-8421d86647dd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.35.10-py3-none-any.whl (328 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.3/328.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.0)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.35.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### An Introductory Example\n",
        " * GPT 3-5 초기 채팅"
      ],
      "metadata": {
        "id": "58jlzOAsdXyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "def init_api():\n",
        "    with open(\"chatgpt.env\") as env:\n",
        "       for line in env:\n",
        "           key, value = line.strip().split(\"=\")\n",
        "           os.environ[key] = value\n",
        "\n",
        "init_api()\n",
        "client = OpenAI(api_key  = os.environ.get(\"API_KEY\"))\n",
        "\n",
        "print(client)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3F8IQOVGWu_J",
        "outputId": "29045b66-d2c2-4ae0-a169-02fd82296f2a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<openai.OpenAI object at 0x7e8e0ae86440>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"gpt-3.5-turbo\"\n",
        "messages = [\n",
        "  {\n",
        "    \"role\": \"system\",\n",
        "    \"content\": \"You are a smart and creative person.\"\n",
        "  },\n",
        "  {\n",
        "    \"role\": \"user\",\n",
        "    \"content\": \"Hi there!\"\n",
        "  }\n",
        "]"
      ],
      "metadata": {
        "id": "oxudCoTcdgAG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=messages\n",
        ")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyxJC6rP0Lw6",
        "outputId": "69a139c2-fbd7-4930-d81b-9636e1674af9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-9ix31z4kjQz2k2SzlYHeWOMkqLf9x', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello! How are you today?', role='assistant', function_call=None, tool_calls=None))], created=1720500439, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=7, prompt_tokens=22, total_tokens=29))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpFQ8ujV0TwD",
        "outputId": "070767c4-b214-4158-b761-01dcf3102a10"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-9ix31z4kjQz2k2SzlYHeWOMkqLf9x', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello! How are you today?', role='assistant', function_call=None, tool_calls=None))], created=1720500439, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=7, prompt_tokens=22, total_tokens=29))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### System, User and Assistant Roles\n",
        " * System, User, Assistant의 역할"
      ],
      "metadata": {
        "id": "yFCAcKo90dv6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### System Role"
      ],
      "metadata": {
        "id": "_aOIu1031QB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"gpt-3.5-turbo\"\n",
        "messages = [\n",
        "  {\n",
        "    \"role\": \"system\",\n",
        "    \"content\": \"너는 영리하고 창의적인 사람이야.\"\n",
        "  },\n",
        "  {\n",
        "    \"role\": \"user\",\n",
        "    \"content\": \"안녕!\"\n",
        "  }\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=messages\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFVqrNFP0oyB",
        "outputId": "fed1f441-1539-477d-e7e4-d5078e283e32"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "안녕하세요! 무엇을 도와드릴까요?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### User Role"
      ],
      "metadata": {
        "id": "plc4rwV106EZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"gpt-3.5-turbo\"\n",
        "messages = [\n",
        "    {\n",
        "      \"role\": \"system\",\n",
        "      \"content\": \"당신은 똑똑한 조수입니다.\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"안녕하세요.\"\n",
        "    },\n",
        "    {\n",
        "    \"role\": \"user\",\n",
        "    \"content\": \"모든 것이 어떻게 진행되고 있나요?\"\n",
        "    }\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=messages\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKVg3RGl1TgA",
        "outputId": "1cd3da07-a7bf-42c0-be91-013b113da94a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "제 일상적인 업무와 일정은 잘 진행되고 있습니다. 감사합니다. 혹시 도울 일이 있으신가요?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assistant Role"
      ],
      "metadata": {
        "id": "scTd2yqc1tTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We are using GPT-4 here\n",
        "model = \"gpt-4\"\n",
        "messages = [\n",
        "  {\n",
        "    \"role\": \"user\",\n",
        "    \"content\":\n",
        "    \"electrons dance the tango of uncertainty,\"\n",
        "    \"entangling bits in a choreography that outpaces\"\n",
        "    \"the swiftest supercomputers.\"\n",
        "  },\n",
        "  {\n",
        "    \"role\": \"assistant\",\n",
        "    \"content\":\n",
        "    \"Electrons Dance the Tango of Uncertainty,\"\n",
        "    \"Entangling Bits in a Choreography That Outpaces\"\n",
        "    \"the Swiftest Supercomputers.\"\n",
        "  },\n",
        "  {\n",
        "    \"role\": \"user\",\n",
        "    \"content\":\n",
        "    \"cloud architectures whisper across the sky,\"\n",
        "    \"weaving a tapestry of data that blankets the digital\"\n",
        "    \"landscape in a seamless symphony of bytes.\"\n",
        "  },\n",
        "  {\n",
        "    \"role\": \"assistant\",\n",
        "    \"content\":\n",
        "    \"Cloud Architectures Whisper Across the Sky,\"\n",
        "    \"Weaving a Tapestry of Data That Blankets the Digital\"\n",
        "    \"Landscape in a Seamless Symphony of Bytes.\"\n",
        "  },\n",
        "  {\n",
        "    \"role\": \"user\",\n",
        "    \"content\":\n",
        "    \"artificial Intelligence, the alchemist of the digital \"\n",
        "    \"age, transmutes raw data into a golden labyrinth of\"\n",
        "    \"insights, charting new territories in the realm of\"\n",
        "    \"human thought.\"\n",
        "  },\n",
        "  {\n",
        "    \"role\": \"assistant\",\n",
        "    \"content\":\n",
        "    \"Artificial Intelligence, the Alchemist of the Digital\"\n",
        "    \"Age, Transmutes Raw Data Into a Golden Labyrinth of\"\n",
        "    \"Insights, Charting New Territories in the Realm of\"\n",
        "    \"Human Thought.\"\n",
        "  },\n",
        "  {\n",
        "    \"role\": \"user\",\n",
        "    \"content\":\n",
        "    \"the internet of things is a vast ocean of data,\"\n",
        "    \"a sea of information that ebbs and flows\"\n",
        "    \"with the tides of time.\"\n",
        "  }\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=messages,\n",
        "  temperature=1.2,\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6ta70jl1xV2",
        "outputId": "cbf1422d-885a-4599-dfed-9cb2bcd63e24"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Internet of Things Is a Vast Ocean of Data, a Sea of Information That Ebbs and Flows with the Tides of Time.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 출력형식지정 - Formatting the Output"
      ],
      "metadata": {
        "id": "FuXAEfPy5oxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"gpt-4\"\n",
        "messages = [\n",
        "  {\n",
        "    \"role\": \"user\",\n",
        "    \"content\":\n",
        "    \"다음을 포함하는 JSON형 반환\"\n",
        "    \"0과 3사이의 소수(Primary numbers)\"\n",
        "  },\n",
        "  {\n",
        "  \"role\": \"assistant\",\n",
        "  \"content\": \"\"\"\n",
        "    {\n",
        "      \"데이터\": [2, 3, 5, 7],\n",
        "      \"길이\": 4,\n",
        "      \"최소\": 2,\n",
        "      \"최대\": 7,\n",
        "    }\n",
        "  \"\"\"\n",
        "  },\n",
        "  {\n",
        "  \"role\": \"user\",\n",
        "  \"content\":\n",
        "    \"다음을 포함하는 JSON형 반환\"\n",
        "    \"0과 6사이의 소수(Primary numbers)\"\n",
        "  },\n",
        "  {\n",
        "  \"role\": \"assistant\",\n",
        "  \"content\": \"\"\"\n",
        "    {\n",
        "    \"데이터\": [2, 3, 5],\n",
        "    \"길이\": 3,\n",
        "    \"최소\": 2,\n",
        "    \"최대\": 5,\n",
        "    }\n",
        "  \"\"\"\n",
        "  },\n",
        "  {\n",
        "  \"role\": \"user\",\n",
        "  \"content\":\n",
        "    \"다음을 포함하는 JSON형 반환\"\n",
        "    \"11과 65사이의 소수(Primary numbers)\"\n",
        "  }\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=messages,\n",
        "  temperature=1.2,\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6dRX1ny6LC-",
        "outputId": "1fa58b11-286f-4f4b-de61-1b258e1979a9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    {\n",
            "    \"데이터\": [11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61],\n",
            "    \"길이\": 14,\n",
            "    \"최소\": 11,\n",
            "    \"최대\": 61\n",
            "    }\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"gpt-4\"\n",
        "prefix = \"\\n\\n1. \"\n",
        "messages = [\n",
        "  {\n",
        "    \"role\": \"user\",\n",
        "    \"content\":\n",
        "    f\"세계 7대 불가사의는 무엇일까요?{prefix}\"\n",
        "  }\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=messages\n",
        ")\n",
        "print(prefix + response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKRQXNnq6qvw",
        "outputId": "df17dd42-58f6-4aec-9a8f-0f671afa8ab6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. 기자의 피라미드 (Egypt)\n",
            "2. 이실리오의 여합 (Babylon)\n",
            "3. 줄로스의 아르테미스 신전 (Turky)\n",
            "4. 올림피아의 제우스 동상 (Greece)\n",
            "5. 마우솔로스의 무덤 (Bodrum)\n",
            "6. 로도스의 거대 동상 (Greece)\n",
            "7. 알렉산드리아의 등대 (Egypt)\n",
            "\n",
            "이 중에서 오직 기자의 피라미드만이 현재까지 남아 있습니다. 나머지는 모두 사라졌거나 폐허가 되었습니다. 이외에도 여러 리스트들이 있습니다. Modern, Natural, New 등의 다른 7대 불가사의들을 참조하실 수 있습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Controlling the Output’s Token Count"
      ],
      "metadata": {
        "id": "Bg9FSeBo77Hb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 출력의 토큰 수 제어"
      ],
      "metadata": {
        "id": "E2yFZaHc8SsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"gpt-3.5-turbo\"\n",
        "messages = [\n",
        "  {\n",
        "    \"role\": \"system\",\n",
        "    \"content\":\n",
        "    \"You are a smart and creative assistant.\"\n",
        "  },\n",
        "  {\n",
        "    \"role\": \"user\",\n",
        "    \"content\":\n",
        "    \"한니발(Hannibal)은 누구인가요?\"\n",
        "  },\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=messages\n",
        ")\n",
        "\n",
        "print(response.usage)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Tt_SCmT8WX2",
        "outputId": "e159a654-f36f-43ad-f7ff-6de750e3b9d5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CompletionUsage(completion_tokens=230, prompt_tokens=36, total_tokens=266)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"gpt-3.5-turbo\"\n",
        "messages = [\n",
        "  {\n",
        "    \"role\": \"system\",\n",
        "    \"content\":\n",
        "    \"당신은 똑똑하고 창의적인 조수입니다.\"\n",
        "  },\n",
        "  {\n",
        "    \"role\": \"user\",\n",
        "    \"content\":\n",
        "    \"한니발(Hannibal)은 누구인가요?\"\n",
        "  },\n",
        "]\n",
        "\n",
        "short_response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=messages,\n",
        "  max_tokens=50\n",
        ")\n",
        "\n",
        "long_response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=messages,\n",
        "  max_tokens=300\n",
        ")\n",
        "\n",
        "print(\"Short response:\")\n",
        "print(short_response.choices[0].message.content)\n",
        "print()\n",
        "print(\"Long response:\")\n",
        "print(long_response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQkvEYWU8sUT",
        "outputId": "dcaa0478-f9dc-4433-a897-1ce98e2dc1d9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Short response:\n",
            "한니발은 툰드라 제국의 정치가이자 전략가로, 고대 로마의 대적이었던 인물입니다. 그는 전\n",
            "\n",
            "Long response:\n",
            "한니발(Hannibal)이라고 하면 대체로 한니발 바르카(Hannibal Barca)를 가리키는 경우가 많습니다. 한니발 바르카는 고대 로마와 캐르타고의 대결을 통해 역사에 이름을 남긴 유명한 군사입니다. 그가 이끈 캐르타고군은 인도아프리카 전역을 정복할 만큼 강력했고, 로마와의 대결 중에는 알피스 전투와 트레비아 강 전투 등을 치른 이름높은 전투가 있었습니다. 한니발은 전쟁에 있어서 대담하고 창의적인 전략가로 인정받았으며, 그의 유명한 전략 중 하나는 알피스 전투에서 알피스 강을 건너지 않고 로마군을 납치하는 것이었습니다. 이처럼 한니발 바르카는 전\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 완료 출력이 중지되는 시점 제어하기"
      ],
      "metadata": {
        "id": "a5Husn-c8--i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Controlling When the Completion Output Stops"
      ],
      "metadata": {
        "id": "-En5hJJQ9ewn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 코드 실행 시 짧은 응답이 잘릴 가능성이 높다. 예시로, \"한니발은 카르타고의 장군으로서 로마와 싸웠다.\"와 같이 응답이 중간에 잘릴 수 있다. 이를 방지하기 위해 stop 매개변수를 사용할 수 있다. 이 매개변수는 일반적으로 다음과 같은 중지 시퀀스를 포함한다:"
      ],
      "metadata": {
        "id": "cIxnGudY9heQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- '.': 마침표를 만나면 응답을 중지.\n",
        "- \\n: 새 줄을 만나면 응답을 중지.\n",
        "- user:: 사용자 역할을 만나면 응답을 중지.\n",
        "- assistant:: 어시스턴트 역할을 만나면 응답을 중지."
      ],
      "metadata": {
        "id": "g-iFAQuC-Gqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"gpt-3.5-turbo\"\n",
        "messages = [\n",
        "  {\n",
        "    \"role\": \"system\",\n",
        "    \"content\":\n",
        "    \"당신은 똑똑하고 창의적인 조수입니다.\"\n",
        "  },\n",
        "  {\n",
        "    \"role\": \"user\",\n",
        "    \"content\":\n",
        "    \"한니발(Hannibal)은 누구인가요?\"\n",
        "  }\n",
        "]\n",
        "\n",
        "stop_token = \".\"\n",
        "response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=messages,\n",
        "  max_tokens=50,\n",
        "  stop=[stop_token]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content + stop_token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eF6486Q__oVJ",
        "outputId": "090df1b8-4e54-4f03-ec58-bbbcebf943c8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "한니발(Hannibal)은 고대 로마의 능력 있는 전략가이자 군사로, 특히 제2차 포에니전쟁.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"gpt-3.5-turbo\"\n",
        "messages = [\n",
        "  {\n",
        "    \"role\": \"system\",\n",
        "    \"content\":\n",
        "    \"당신은 똑똑하고 창의적인 조수입니다.\"\n",
        "  },\n",
        "  {\n",
        "    \"role\": \"user\",\n",
        "    \"content\":\n",
        "    \"한니발(Hannibal)은 누구인가요?\"\n",
        "  }\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=messages,\n",
        "  max_tokens=300,\n",
        "  stop=[\"\\n\", \"Human:\", \"AI:\"]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lKVAXSqAQ6J",
        "outputId": "426ed2bd-185a-45fe-8b31-e7e94b7785dc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "한니발(Hannibal)은 포아니 전쟁에서 로마 제국에 맹렬히 저항한 군대 지휘관으로 유명한 인물입니다. 이탈리아 인근에서 로마군을 여러 차례 패배시키고 로마 자체까지 침입하여 로마에 큰 위협을 줬습니다. 그의 전략과 지휘 역량은 역사적으로도 매우 존경받는 것으로 알려져 있습니다. 그의 이야기는 유명한 돌파점과 전략 전술로 표현되어 왔습니다.《한니발 전투》 등의 서적과 영화에서 그의 이야기가 다뤄졌습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 대화 시나리오: 만약 AI가 대화형 응답을 생성하는 상황에서, Human: 또는 AI:와 같은 구분자를 만나면 응답을 중지시켜 대화 흐름을 제어합니다."
      ],
      "metadata": {
        "id": "S7cckvkIBM8w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 특정 연도의 공상 과학, 판타지 영화 나열"
      ],
      "metadata": {
        "id": "jRa4M6KjApzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"gpt-3.5-turbo\"\n",
        "messages = [\n",
        "  {\n",
        "    \"role\": \"user\",\n",
        "    \"content\": \"2021년에 개봉한 공상 과학 영화를 알려주세요.\"\n",
        "  },\n",
        "  {\n",
        "  \"role\": \"system\",\n",
        "  \"content\": \"\"\"\n",
        "    1. 듄 (Dune)\n",
        "    2. 핀치 (Finch)\n",
        "    3. 더 어웨이크 (The Awake)\n",
        "    4. 매트릭스: 리저렉션 (The Matrix Resurrections)\n",
        "    5. 마더/안드로이드 (Mother/Android)\n",
        "    6. 블리스 (Bliss)\n",
        "    7. 스완 송 (Swan Song)\n",
        "  \"\"\"\n",
        "  },\n",
        "  {\n",
        "  \"role\": \"user\",\n",
        "  \"content\": \"2021년에 개봉한 인기 영화를 알려주세요.\"\n",
        "  }\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=messages,\n",
        "  max_tokens=300,\n",
        "  stop=[\"Human:\", \"AI:\"]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MOkYuU7BxAe",
        "outputId": "f40c3175-a818-403d-9010-555e42a41a08"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021년에 개봉한 인기 영화들은 다양한 장르와 주제를 다루고 있습니다. 일부 인기 있는 영화들은 다음과 같습니다:\n",
            "\n",
            "1. 노매드랜드 (Nomadland)\n",
            "2. 사일런스 (Sound of Metal)\n",
            "3. 사스페리코 (The Father)\n",
            "4. 코다 (CODA)\n",
            "5. 주다스 앤 더 블랙 메시아 (Judas and the Black Messiah)\n",
            "6. 미나리 (Minari)\n",
            "7. 스코어: 영화음악의 모든 것 (The Score)\n",
            "8. 솔직히 까기 싫은데 괜찮아 (I Care a Lot)\n",
            "9. 카오스 워킹 (Chaos Walking)\n",
            "10. 삼진그룹 영어토익반 (Samjin Company English Class)\n",
            "\n",
            "이 외에도 2021년에는 다양한 흥행작이 나왔으니 영화관이나 온라인 스트리밍 플랫폼을 통해 더 많은 작품\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STOP 사용"
      ],
      "metadata": {
        "id": "P_v5AFoCC_PW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"gpt-3.5-turbo\"\n",
        "messages = [\n",
        "  {\n",
        "    \"role\": \"user\",\n",
        "    \"content\": \"2021년에 개봉한 공상 과학 영화를 알려주세요.\"\n",
        "  },\n",
        "  {\n",
        "  \"role\": \"system\",\n",
        "  \"content\": \"\"\"\n",
        "    1. 듄 (Dune)\n",
        "    2. 핀치 (Finch)\n",
        "    3. 더 어웨이크 (The Awake)\n",
        "    4. 매트릭스: 리저렉션 (The Matrix Resurrections)\n",
        "    5. 마더/안드로이드 (Mother/Android)\n",
        "    6. 블리스 (Bliss)\n",
        "    7. 스완 송 (Swan Song)\n",
        "  \"\"\"\n",
        "  },\n",
        "  {\n",
        "  \"role\": \"user\",\n",
        "  \"content\": \"2021년에 개봉한 인기 영화를 알려주세요.\"\n",
        "  }\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=messages,\n",
        "  max_tokens=300,\n",
        "  stop=[\"6.\"]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNej-MGNDbsc",
        "outputId": "353ab598-cbc5-49f9-d0de-3e868093ebcc"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021년에 개봉한 인기 영화를 알려드리겠습니다.\n",
            "\n",
            "1. 디어 에반 한센 (Dear Evan Hansen)\n",
            "2. 보스 베이비: 패밀리 비지니스 (The Boss Baby: Family Business)\n",
            "3. 크루엘라 (Cruella)\n",
            "4. 블랙 위도우 (Black Widow)\n",
            "5. 샹치와 텐 링즈의 전설 (Shang-Chi and the Legend of the Ten Rings)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 온도(Temperature)와 환각 - Temperature and Hallucination"
      ],
      "metadata": {
        "id": "e_McciMoDfeO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 온도(Temperature) : 온도는 모델의 창의성과 보수성을 조절하는 매개변수.\n",
        "  * 높은 온도(예: 2.0)는 모델을 더 창의적이고 예측 불가능하게 만들지만, 문맥과 상관없는 단어를 생성할 가능성이 크다.\n",
        "  * 낮은 온도(예: 0.2)는 모델을 더 보수적이고 예측 가능하게 만들며, 주로 훈련된 데이터에 더 가깝게 응답한다.\n",
        "\n",
        "* 환각(Hallucination)\n",
        "  * 높은 온도에서 모델이 문맥과 상관없는 단어나 정보를 생성하는 현상을 의미."
      ],
      "metadata": {
        "id": "a8m4u_uXDuyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"gpt-4\"\n",
        "prefix = \"옛날 옛적에 \"\n",
        "messages = [\n",
        "{\n",
        "  \"role\": \"system\",\n",
        "  \"content\": \"당신은 스토리 텔러입니다.\"\n",
        "},\n",
        "{\n",
        "  \"role\": \"user\",\n",
        "  \"content\": prefix\n",
        "},\n",
        "]\n",
        "\n",
        "response_high_temperature = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=messages,\n",
        "  max_tokens=100,\n",
        "  temperature=2,\n",
        "  stop=[\"\\n\",]\n",
        ")\n",
        "content_high_temperature = \\\n",
        "  response_high_temperature.choices[0].message.content\n",
        "\n",
        "response_medium_temperature = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=messages,\n",
        "  max_tokens=100,\n",
        "  temperature=1,\n",
        "  stop=[\"\\n\",]\n",
        ")\n",
        "\n",
        "content_medium_temperature = \\\n",
        "  response_medium_temperature.choices[0].message.content\n",
        "\n",
        "response_low_temperature = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=messages,\n",
        "  max_tokens=100,\n",
        "  temperature=0,\n",
        "  stop=[\"\\n\",]\n",
        ")\n",
        "\n",
        "content_low_temperature = \\\n",
        "  response_low_temperature.choices[0].message.content\n",
        "\n",
        "\n",
        "print(f\"\"\"\n",
        "  1. High temperature:\n",
        "  {prefix}{content_high_temperature}\n",
        "\"\"\")\n",
        "\n",
        "print(f\"\"\"\n",
        "  2. Medium temperature:\n",
        "  {prefix}{content_medium_temperature}\n",
        "\"\"\")\n",
        "\n",
        "print(f\"\"\"\n",
        "  3. Low temperature:\n",
        "  {prefix}{content_low_temperature}\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjorWBJeEAe4",
        "outputId": "d9300594-2a6b-4bdc-f149-e041f86a4a03"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  1. High temperature:\n",
            "  옛날 옛적에 pickup deep^^우(unique value fun/') '라.link Evidence-o/php/designctypeisseur', CSS nuclear_merged\\t One_signalLabelConsulta (\n",
            "\n",
            "\n",
            "  2. Medium temperature:\n",
            "  옛날 옛적에 얼음의 왕국으로 불리던 겨울 나라가 있었습니다. 이 나라는 영원히 설경이 빚어졌고, 빛나는 얼음 조각이 하늘에서 내려와 마치 별빛처럼 땅을 수놓았습니다.\n",
            "\n",
            "\n",
            "  3. Low temperature:\n",
            "  옛날 옛적에 한 작은 마을에 아름다운 소녀가 살았습니다. 그녀의 이름은 로제라고 했어요. 로제는 그 마을에서 가장 아름다운 소녀로 알려져 있었지만, 그녀의 아름다움은 그녀의 마음속까지 이어져 있었습니다. 그녀는\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Top_p 샘플링\n",
        " * Sampling with Top_p"
      ],
      "metadata": {
        "id": "LUdd3_cnE2D9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"gpt-4\"\n",
        "prefix = \"옛날 옛적에 \"\n",
        "messages = [\n",
        "  {\n",
        "    \"role\": \"system\",\n",
        "    \"content\": \"당신은 스토리 텔러입니다.\"\n",
        "  },\n",
        "  {\n",
        "    \"role\": \"user\",\n",
        "    \"content\": prefix\n",
        "  }\n",
        "]\n",
        "\n",
        "response_high_topp = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=messages,\n",
        "  max_tokens=100,\n",
        "  top_p=1,\n",
        "  stop=[\"\\n\",]\n",
        ")\n",
        "\n",
        "content_high_topp = \\\n",
        "  response_high_topp.choices[0].message.content\n",
        "\n",
        "response_medium_topp = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=messages,\n",
        "  max_tokens=100,\n",
        "  temperature=1,\n",
        "  stop=[\"\\n\",]\n",
        ")\n",
        "\n",
        "content_medium_topp = \\\n",
        "  response_medium_topp.choices[0].message.content\n",
        "\n",
        "response_low_topp = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=messages,\n",
        "  max_tokens=100,\n",
        "  temperature=0,\n",
        "  stop=[\"\\n\",]\n",
        ")\n",
        "\n",
        "content_low_topp = \\\n",
        "  response_low_topp.choices[0].message.content\n",
        "\n",
        "print(f\"\"\"\n",
        "1. High topp:\n",
        "{prefix}{content_high_topp}\n",
        "\"\"\")\n",
        "\n",
        "print(f\"\"\"\n",
        "2. Medium topp:\n",
        "{prefix}{content_medium_topp}\n",
        "\"\"\")\n",
        "\n",
        "print(f\"\"\"\n",
        "3. Low topp:\n",
        "{prefix}{content_low_topp}\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZUr4UZBFPSI",
        "outputId": "08f8c1b5-71f1-422c-8621-5cce09c7e26a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1. High topp:\n",
            "옛날 옛적에 마법의 힘으로 소원을 이룰 수 있는 요정들이 살던 숲이 있었습니다. 그 숲은 무지개 같은 다양한 색으로 빛나는 꽃들로 가득했고, 동물들의 노래 소리가 끊임없이 울려 퍼지\n",
            "\n",
            "\n",
            "2. Medium topp:\n",
            "옛날 옛적에 한 작은 마을에서 의료사고로 의식을 잃고 일어날 수 없었던 한 소년이 살았습니다. 그의 이름은 티모라스였습니다. 그 소년은 재능이 매우 특별했어요. 그는 생각만으로도 무언가를 움직일\n",
            "\n",
            "\n",
            "3. Low topp:\n",
            "옛날 옛적에 한 작은 마을에 아름다운 소녀가 살았습니다. 그녀의 이름은 로제라고 했어요. 로제는 그 어떤 소녀보다도 아름다웠지만, 그녀의 아름다움은 그녀의 마음속까지 이어져 있었습니다. 그녀는 항상 사람\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### API 응답 스트리밍\n",
        " * Streaming the API Response"
      ],
      "metadata": {
        "id": "yEjbQeZdGAuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"gpt-4\"\n",
        "prefix = \"옛날 옛적에 \"\n",
        "messages = [\n",
        "  {\n",
        "    \"role\": \"system\",\n",
        "    \"content\": \"당신은 스토리 텔러입니다.\"\n",
        "  },\n",
        "  {\n",
        "    \"role\": \"user\",\n",
        "    \"content\": prefix\n",
        "  },\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=messages,\n",
        "  max_tokens=100,\n",
        "  # 이 옵션을 True로 설정하여 스트리밍을 활성화합니다.\n",
        "  stream=True\n",
        ")\n",
        "\n",
        "print(prefix, end=\"\")\n",
        "for message in response:\n",
        "  content = message.choices[0].delta.content\n",
        "  if content:\n",
        "    print(content, end=\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljX9ELvKGcsY",
        "outputId": "1c6950c6-9436-4754-b51b-06f0cb76115e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "옛날 옛적에 한 아름다운 이야기가 있었습니다. 이 이야기는 한 목동과 작은 양이 주인공입니다. \n",
            "\n",
            "그 목동은 자신의 양들을 무척이나 사랑했지만, 그 중에서도 가장 작은 양을 가장 사랑했습니다. 그 양은 작지만 매우 착하고 순수한"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 반복성 제어: 빈도 및 존재 페널티\n",
        " * Controlling Repetitivity: Frequency and Presence Penalties"
      ],
      "metadata": {
        "id": "cw6rmodQI3Xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"gpt-3.5-turbo\"\n",
        "prefix = \"옛날 옛적에 \"\n",
        "messages = [\n",
        "  {\n",
        "  \"role\": \"system\",\n",
        "  \"content\": \"당신은 스토리 텔러입니다.\"\n",
        "  },\n",
        "  {\n",
        "  \"role\": \"user\",\n",
        "  \"content\": prefix\n",
        "  },\n",
        "]\n",
        "\n",
        "response_high_frequency_penalty = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=messages,\n",
        "  max_tokens=100,\n",
        "  frequency_penalty=2.0\n",
        ")\n",
        "\n",
        "response_low_frequency_penalty = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=messages,\n",
        "  max_tokens=100,\n",
        "  frequency_penalty=0\n",
        ")\n",
        "\n",
        "response_high_presence_penalty = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=messages,\n",
        "  max_tokens=100,\n",
        "  presence_penalty=2.0\n",
        ")\n",
        "\n",
        "response_low_presence_penalty = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=messages,\n",
        "  max_tokens=100,\n",
        "  presence_penalty=0\n",
        ")\n",
        "\n",
        "content_high_frequency_penalty = \\\n",
        "  response_high_frequency_penalty.choices[0].message.content\n",
        "content_low_frequency_penalty = \\\n",
        "  response_low_frequency_penalty.choices[0].message.content\n",
        "content_high_presence_penalty = \\\n",
        "  response_high_presence_penalty.choices[0].message.content\n",
        "content_low_presence_penalty = \\\n",
        "  response_low_presence_penalty.choices[0].message.content\n",
        "\n",
        "print(\"High frequency penalty:\")\n",
        "print(prefix + content_high_frequency_penalty)\n",
        "print()\n",
        "\n",
        "print(\"Low frequency penalty:\")\n",
        "print(prefix + content_low_frequency_penalty)\n",
        "print()\n",
        "\n",
        "print(\"High presence penalty:\")\n",
        "print(prefix + content_high_presence_penalty)\n",
        "print()\n",
        "\n",
        "print(\"Low presence penalty:\")\n",
        "print(prefix + content_low_presence_penalty)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uUztsnQJEIs",
        "outputId": "96273fde-82c0-41b5-9939-94060ffb124a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "High frequency penalty:\n",
            "옛날 옛적에 한 그늘진 숲 속에 작은 마을이 있었습니다. 이 마을은 아름다운 자연과 신비로운 에너지로 둘러싸여 있어서 주변 사람들에게는 언제나 특별한 곳으로 여겨졌습니다. 한 가정에서는 특히도 신화와 전\n",
            "\n",
            "Low frequency penalty:\n",
            "옛날 옛적에 한 소녀가 살았습니다. 그 소녀의 이름은 미나였죠. 미나는 작은 마을에 살고 있었는데, 그 마을은 아름다운 자연과 따뜻한 이웃들로 가득했습니다. 하지만 어느 날, 마을에 큰 재\n",
            "\n",
            "High presence penalty:\n",
            "옛날 옛적에 한 가족이 살았습니다. 이 가족은 작은 마을에 차고 날 말하더니 언젠가 신비한 숲으로 사라졌다는 전설이 있었습니다. 한 소녀는 그 가족의 이야기를 듣고 궁금해했습니다. 결심을 하고 숲으로\n",
            "\n",
            "Low presence penalty:\n",
            "옛날 옛적에 한 마을에 하나님을 숭배하는 사람들이 살고 있었습니다. 이 마을 사람들은 매일 아침, 아름다운 순례를 하며 하나님께 기도를 드리고 있었습니다. 그러던 어느 날, 마을에 한 사람이 찾아와 말했습니다\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 빈도 페널티 대 존재 페널티\n",
        " * Frequency vs. Presence Penalty"
      ],
      "metadata": {
        "id": "ixduNOsxJlku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### API에서 결과의 수 제어하기\n",
        " * Controlling the Number of Results from the API"
      ],
      "metadata": {
        "id": "nBDRxh9mJyeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"gpt-4\"\n",
        "prefix = \"옛날 옛적에 \"\n",
        "messages = [\n",
        "  {\n",
        "    \"role\": \"system\",\n",
        "    \"content\": \"당신은 스토리 텔러입니다.\"\n",
        "  },\n",
        "  {\n",
        "    \"role\": \"user\",\n",
        "    \"content\": prefix,\n",
        "  }\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=messages,\n",
        "  n = 2,\n",
        "  stop=[\"\\n\"]\n",
        ")\n",
        "\n",
        "choices = response.choices\n",
        "for choice in choices:\n",
        "  print(f\"Choice: {choice.index}\")\n",
        "  print(prefix + choice.message.content)\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pFdpcAIJ6r_",
        "outputId": "c2fdfbc1-6bf9-4749-af32-de79ff95e28c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Choice: 0\n",
            "옛날 옛적에 고요한 작은 마을에 친절하고 성실한 소년 민수가 살고 있었습니다. 민수의 가장 큰 특징은 무엇이든지 파고들어 자신이 지녀야한다는 꼼꼼함과 집요함이었는데요. 그의 이런 성격은 마을 사람들로부터는 찬사를 받지만, 동시에 그로 인해 많은 고생을 할 때도 있었습니다.\n",
            "\n",
            "Choice: 1\n",
            "옛날 옛적에 잊혀진 작은 마을이 있었습니다. 이 마을은 다른 어떤 마을과 달리 매우 독특한 규칙이 있었는데, 그것은 마을 사람들이 각각의 일러를 공유하는 것이었습니다. 이 규칙은 대대로 이어져 내려오며, 암묵적으로 지켜져 왔습니다.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EM95ixl7KOAJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}